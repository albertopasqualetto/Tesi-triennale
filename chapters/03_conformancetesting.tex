%!TEX root = ../main.tex

\chapter{Conformance Testing} \label{chp:conformancetesting}    % a che serve, con specifica ambiente di sviluppo (python, poetry, git, gitlab DEI)
I test di conformità, come anticipato nella sezione \ref{sec:standard-mpai}, sono un insieme di attività di verifica che determinano l'aderenza di un processo, prodotto o servizio a dei requisiti tecnici o a delle norme.
Nel caso in esame, \ac{ARP}, si tratta di verificare che il suo funzionamento segua, entro certi limiti (necessari anche perchè è una IA), le specifiche tecniche. In \ac{MPAI}, questi test, sono definiti all'interno di un documento apposito.

L'obiettivo di questa tesi è quella di scrivere il codice dei test di conformità, basandosi sul documento che li descrive fornito da MPAI.

L'ambiente di sviluppo è un ambiente virtuale di \href{https://python-poetry.org/}{Poetry} basato su Python 3.10 (poi aggiornato a 3.11)\footnote{Vedi sezione \ref{ssec:py-311}}, su una macchina Windows 11 (per alcuni confronti, occasionalmente è stata utilizzata una macchina virtuale con Ubuntu 20.04.6 LTS). L'ambiente di produzione è un server Docker con container linux.\footnote{Per questo motivo l'implementazione del CSC ha anche una modalità di esecuzione come server con protocollo gRPC, utilizzato per fare comunicare i vari \acp{AIM} tra loro.} Per il versionamento del software viene usato \href{https://git-scm.com/}{git}, utilizzando come server il \href{https://gitlab.dei.unipd.it/}{GitLab del \ac{DEI}}.\footnote{Le repositories rilvanti ai fini di questo documento sono: \href{https://gitlab.dei.unipd.it/mpai/packager}{\ac{AIM} Packager}; \href{https://gitlab.dei.unipd.it/mpai/audio-analyzer}{\ac{AIM} Audio Analyser}; \href{https://gitlab.dei.unipd.it/mpai/mpai-cae-arp}{libreria mpai-cae-arp}.}


\section{Tests e \acl{TDD}} \label{sec:tests-tdd}
Il software testing è il processo di valutazione e verifica del corretto funzionamento di un prodotto software rispetto alle aspettative; la creazione di test suites ha l'obiettivo di rilevare bug prima di rilasciare il prodotto.
Solitamente si tende ad automatizzare i test attraverso alcuni framework in modo tale da poterli eseguire ad ogni modifica del codice.

Il \acfi{TDD} è un approccio allo sviluppo di software che prevede la scrittura dei test prima di quella del codice ai quali deve esserne sottoposto; inoltre i test sono da ripetere man mano che il software viene sviluppato.
I test di conformità sono il documento ed i test stessi che l'implementazione software delle specifiche tecniche dovranno rispettare, quindi la loro scrittura e comunque una correzione del codice basandosi su di essi può essere riconosciuta come un approccio di \ac{TDD}. % TODO vedere se dare più importanza


\subsection{Pytest} \label{ssec:pytest} % funzionamento/utlità, xdist per parallelizzare, json-report per scrivere il report richiesto, fixtures per eseguire funzione per ogni file
Uno dei framework per il testing in Python più popolari è \href{https://pytest.org}{pytest}, esso permette di ottenere informazioni dettagliate sul fallimento degli \texttt{assert} statements\footnote{\texttt{assert} è la parola chiave che permette di effettuare i test, nello specifico il test procede se i suoi parametri sono \texttt{True}, mentre viene lanciato \texttt{AssertionError} se i suoi parametri sono \texttt{False}.}, di avere fixtures\footnote{Una \textit{fixture} è un elemento del software testing che viene utilizzato per definire un contesto per l'esecuzione di uno (o più) test.} modulari e di essere compatibile con numerosi plugin esterni.

\href{https://github.com/numirias/pytest-json-report}{pytest-json-report} è un plugin che è stato utilizzato per creare i report richiesti come output del conformance testing in formato JSON.

\href{https://pytest-xdist.readthedocs.io/}{pytest-xdist} è un plugin che è stato utilizzato per parallelizzare l'esecuzione (vedi sezione \ref{sec:parallelizzazione}

Sono state utilizzate delle fixture per definire l'ambiente di test e per ottenere delle cartelle di test (\verb|pytest_sessionstart|, \verb|pytest_sessionfinish| e \verb|tmp_path|), inoltre è stata parametrizzata l'esecuzione delle varie funzioni di test in modo tale da essere eseguite per ogni documento digitalizzato tramite il decoratore \texttt{@pytest.mark.parametrize}.


\section{\acs{CAE}-\acs{ARP} packager} \label{sec:test-packager}
Il primo \ac{AIM} preso in esame è stato il packager.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{img/packager.png}
    \caption{\ac{ARP} Packager}
    \label{fig:packager}
\end{figure}

% \begin{table}[h]
%     \centering
%     \begin{tabular}{|c|c|}
%         \hline
%         \textbf{Input data}             &   \textbf{Output Data}\\
%         \hline
%         Preservation Audio File         &   Access Copy Files\\
%         Preservation Audio-Visual File  &   Preservation Master Files\\
%         Restored Audio Files            &   \\
%         Editing List                    &   \\
%         Irregularity File               &   \\
%         Irregularity Images             &   \\
%         \hline
%     \end{tabular}
%     \caption{I/O di \ac{ARP} Packager}
%     \label{tab:packager-io}
% \end{table}

Il packager, come anticipato nella sottosezione \ref{ssec:mpai-cae-arp}, è l'ultimo \ac{AIM} ad essere eseguito e si occupa di raccogliere tutti i file elaborati e di restituire in uscita all'\ac{AIW} una cartella con la copia d'accesso dei file ed una con i file grezzi accompagnati da tutte le irregolarità trovate, vedi schema in figura \ref{fig:packager}.

Viene riportata la descrizione dei conformance tests da seguire, definita da MPAI, nella tabella \ref{tab:packager-valutazione}.\footnote{Estratta dalla versione WD 0.15.2 del documento MPAI Conformance Testing per \ac{CAE}.} 

\begin{table}[h]
    \centering
    \begin{tabular}{|p{0.2\textwidth}|p{0.8\textwidth}|}
        \hline
        \textbf{Means}   &   \textbf{Actions}\\
        \hline
        \textbf{Conformance Testing Dataset}    &
            DS1: n Preservation Audio Files.\newline
            DS2: n Preservation Audio-Visual Files related to DS1.\newline
            DS3: n Restored Audio Files arrays related to DS1 coming from Tape Audio Restoration.\newline
            DS4: n Editing Lists related to DS3 coming from Tape Audio Restoration.\newline
            DS5: n Irregularity Files related to DS1 coming from Tape Irregularity Classifier.\newline
            DS6: n Irregularity Images related to DS5 coming from Tape Irregularity Classifier.\newline
            DS7: n Access Copy Files.\newline
            DS8: n Preservation Master Files.\\
        \hline
        \textbf{Procedure}  &
            1.	Feed Packager under test with DS1, DS2, DS3, DS4, DS5 and DS6.\newline
            2.	Compare the output Access Copy Files with DS7.\newline
            3.	Compare the output Preservation Master Files with DS8.\\
        \hline
        \textbf{Evaluation} &
            For a given input tuple, verify that:\newline
                1.	The output Access Copy Files contain the Restored Audio Files, the Editing List, the Irregularity File and the set of Irregularity Images in a .zip file, and is therefore equal to DS7.\newline
                2.	The output Preservation Master Files contain the Preservation Audio File, the Preservation Audio-Visual File with the audio of the Preservation Audio File, the Irregularity File and the Irregularity Images, and is therefore equal to DS8.\newline
            An error on any of the output arrays will make the Packager under test not conformant.\\
        \hline
    \end{tabular}
    \caption{Test di conformità per \ac{ARP} Packager}
    \label{tab:packager-valutazione}
\end{table}

Ad ogni esecuzione dei test è richiesto di inserire i dati nella tabella \ref{tab:packager-report}.
\begin{table}[H]
    \centering
    \begin{adjustbox}{max width=\textwidth}
        \begin{tabular}{|c|c|}
            
            \hline
            \textbf{Conformance Tester ID}                      &   Unique Conformance Tester Identifier assigned by MPAI\\
            \hline
            \textbf{Standard, Use Case ID and Version}          &   Standard ID and Use Case ID, Version and Profile of the standard in the form “CAE:ARP:1:0”.\\
            \hline
            \textbf{Name of AIM}                                &   Packager\\
            \hline
            \textbf{Implementer ID}                             &   Unique Implementer Identifier assigned by MPAI Store.\\
            \hline
            \textbf{AIM Implementation Version}                 &   Unique Implementation Identifier assigned by Implementer.\\
            \hline
            \textbf{Neural Network Version (optional)}          &   Unique Neural Network Identifier assigned by Implementer.\\
            \hline
            \textbf{Identifier of Conformance Testing Dataset}  &   Unique Dataset Identifier assigned by MPAI Store.\\
            \hline
            \textbf{Test ID}                                    &   Unique Test Identifier assigned by Conformance Tester.\\
            \hline
            \textbf{Actual output}                              &   Actual output provided as a matrix of n rows containing output assertions.\\
                                                                &   \begin{tabular}{|c|c|c|c|c|}
                \hline
                Output                      &   Files                           &   1   &   \dots   &   n\\
                \hline
                Access Copy Files           &   Restored Audio Files            &   T/F &   \dots   &   T/F\\
                                            &   Editing List                    &   T/F &   \dots   &   T/F\\
                                            &   Irregularity File               &   T/F &   \dots   &   T/F\\
                                            &   Irregularity Images             &   T/F &   \dots   &   T/F\\
                \hline
                Preservation Master Files   &   Preservation Audio File         &   T/F &   \dots   &   T/F\\
                                            &   Preservation Audio Visual File  &   T/F &   \dots   &   T/F\\
                                            &   Irregularity File               &   T/F &   \dots   &   T/F\\
                                            &   Irregularity Images             &   T/F &   \dots   &   T/F\\
                \hline
            \end{tabular}\\
                                                                &   Final assertion: T/F\\
            \hline
            \textbf{Execution time (optional)}                  &   Duration of test execution\\
            \hline
            \textbf{Test comment (optional)}                    &   Comments on test results and possible needed actions.\\
            \hline
            \textbf{Test Date}                                  &   yyyy/mm/dd.\\
            \hline
        \end{tabular}
    \end{adjustbox}
    \caption{Report da compilare ad ogni esecuzione del packager.}
    \label{tab:packager-report}
\end{table}
Viene creato automaticamente il report sotto forma di file JSON contenente le informazioni conosciute dal software anche grazie all'aiuto della libreria \href{https://github.com/numirias/pytest-json-report}{pytest-json-report} che raccoglie i risultati dei test in un formato machine-readable.

Il seguente listato ne è un esempio:
\lstinputlisting[language=json]{listings/packager-report.json}


\subsection{Bug ed altri problemi pre-esistenti} \label{ssec:packager-pre}  % moviepy vs ffmpeg (errori nel video e audio transcodifica in mp3), offset
Prima di dedicarsi alla scrittura dei test, si è verificata la corretta funzionalità del software.

Sono stati riscontrati dei problemi importanti relativi alla creazione del \textit{PreservationAudioVisualFile}

\begin{lstlisting}[language=Python, caption={Codice iniziale, creazione PreservationAudioVisualFile}]
# Create Preservation Audio-Visual File with substituted audio
video_file = files_name + '.mov'
pvf_path = os.path.join(working_path, 'PreservationAudioVisualFile/', video_file)
try:
    audio = AudioFileClip(paf_path)
    video = VideoFileClip(pvf_path)
    # Open Irregularity File to get offset
    irregularity_file_json = open(
        os.path.join(temp_path, 'TapeIrregularityClassifier_IrregularityFileOutput2.json')
    )
    irregularity_file = json.load(irregularity_file_json)
    offset = irregularity_file['Offset']/1000
    if offset > 0:
        audio = audio.subclip(t_start=offset)
    else:
        video = video.subclip(t_start=offset)
    video = video.set_audio(audio)
    video.write_videofile(pmf_path + 'PreservationAudioVisualFile.mov', bitrate='3000k', codec='mpeg4')
    print("Preservation Audio-Visual File created")
except OSError:
    pprint(f"Preservation Audio-Visual File file '{pvf_path}' not found!", color=Color.RED)
    quit(os.EX_NOINPUT)
\end{lstlisting}

Il primo problema è relativo alla sincronizzazione del video con l'audio: l'audio deve essere anticipato se viene trovato un offset positivo, altrimenti deve essere anticipato il video.
Alle righe 12-16, si osserva che il comportamento non è quello richiesto, dato che a riga 16 l'offset ha valore negativo, allora \href{https://zulko.github.io/moviepy/}{MoviePy}, libreria utilizzata per eseguire editing video, farà iniziare la traccia a partire da $|offset|$ secondi prima del termine della clip.\footnote{Codice sorgente: \url{https://zulko.github.io/moviepy/_modules/moviepy/Clip.html\#Clip.subclip}}    % TODO da verificare se vero

Il secondo problema è relativo alla non specifica nel codice della codifica audio che porta la libreria MoviePy a ricadere nel comportamento di default, ovvero generare un file con codifica MP3 (lossy) a \qty{44100}{\Hz} e ad avere di conseguenza, in questo caso, una transcodifica, il che non è ottimale per un software che ha come obiettivo la conservazione di documenti audio.

Il terzo problema invece è provocato dalla libreria \href{https://zulko.github.io/moviepy/}{MoviePy} che, per motivi sconosciuti e solo in alcuni casi, genera file corrotti nella traccia video, nello specifico che si bloccano dopo alcuni secondi.  % TODO verificare se c'entra con il problema di cui sopra.

Per risolvere questi due problemi si è scelto di sostituire MoviePy direttamente con \href{https://ffmpeg.org/}{FFmpeg}, ottenendo come risultato il seguente codice:
\begin{lstlisting}[language=Python, caption={Codice finale, creazione PreservationAudioVisualFile}]
# Create Preservation Audio-Visual File with substituted audio
video_file = files_name + '.mov'
pvf_path = os.path.join(working_path, 'PreservationAudioVisualFile/', video_file)
try:
    # Open Irregularity File to get offset
    irregularity_file_json = open(
        os.path.join(temp_path, 'TapeIrregularityClassifier_IrregularityFileOutput2.json')
    )
    irregularity_file = json.load(irregularity_file_json)
    offset = irregularity_file['Offset']
    command_to_run = ['ffmpeg',
                      '-y', '-hide_banner', '-loglevel', 'error']
    # If offset is positive, the audio is anticipated, otherwise video is anticipated (through seek)
    if offset > 0:
        command_to_run = command_to_run + ['-i', pvf_path,
                                           '-ss', str(offset)+'ms', '-i', paf_path]
    else:
        command_to_run = command_to_run + ['-ss', str(offset * -1)+'ms', '-i', pvf_path,
                                           '-i', paf_path]
    command_to_run = command_to_run + ['-c:v', 'mpeg4', '-c:a', 'copy',
                                       '-map', '0:v', '-map', '1:a',
                                       '-b:v', '3M', '-maxrate', '4M', '-bufsize', '4M',
                                       pmf_path + 'PreservationAudioVisualFile.mov']
    subprocess.run(command_to_run)
    print("Preservation Audio-Visual File created")
except OSError:
    pprint(f"Preservation Audio-Visual File file '{pvf_path}' not found!", color=Color.RED)
    quit(os.EX_NOINPUT)
\end{lstlisting}


\subsection{Come verificare l'uguaglianza tra audio} \label{ssec:packager-audio}    % fingerprinting con chromaprint, suo wrapper in python, free software e mie contribuzioni, comunicare col mantainer
Per verificare l'uguaglianza fra tracce audio, inizialmente si è provato un confronto tramite le librerie \textit{hashlib} o \textit{filecmp}, ma entrambi i test non hanno avuto successo perchè in alcuni casi non è richiesto di verificare che il file sia lo stesso identico (stesso hash), ma piuttosto di controllare che il suono sia il medesimo\footnote{La differenza si osserva, per esempio, quando si hanno 2 file, uno scostato di qualche secondo rispetto all'altro, questo è ciò che accade nel caso in esame, quando si esegue la sincronizzazione audio/video.}.

La soluzione trovata si basa sulla libreria \href{https://acoustid.org/chromaprint}{chromaprint}\footnote{chromaprint è la libreria che è alla base del progetto AcoustID, utilizzato nel piuttosto famoso software \href{https://musicbrainz.org/}{MusicBrainz} (\href{https://picard.musicbrainz.org/}{Picard}) che serve a taggare le proprie tracce musicali.}, la quale permette di ricavare delle impronte digitali (\textit{fingerprint}) da delle tracce audio, in maniera da poi confrontarle tra loro per ottenere la loro somiglianza.    % TODO taggare va bene?

È stata utilizzata in particolare una libreria Python, la quale non è altro che un wrapper di AcoustID, chiamata \href{https://github.com/beetbox/pyacoustid}{pyacoustid} che aiuta lo sviluppatore Python esponendo direttamente le API di chromaprint ed un metodo per confrontare le impronte.

\begin{lstlisting}[language=Python, caption=Test di comparazione di due file audio tramite la loro impronta digitale]
AUDIO_THRESHOLD = 0.7   # (ratio)
# [...]
input_fingerprint = acoustid.fingerprint_file(input_paf_path)
output_fingerprint = acoustid.fingerprint_file(output_paf_path)
assert acoustid.compare_fingerprints(input_fingerprint, output_fingerprint) > AUDIO_THRESHOLD, "PreservationAudioFile.wav is not the same as input"
\end{lstlisting}

Come si può leggere dal codice, è stata scelta come soglia di somiglianza il $70\%$, non è il $100\%$ perchè, ad esempio, nei casi in cui è stata effettuata la sincronizzazione audio/video l'audio non è esattamente uguale, inoltre dai vari test eseguiti sui dataset forniti, questo valore ha funzionato correttamente.

Durante la scelta della soglia da adottare sono state eseguite diverse prove ed è stato verificato che, correttamente, nel caso dei file in esame, si ottengono dei risultati elevai, talvolta uguali a $1$, mentre in caso di file diversi, si ottiene un valore tendente a $0$.\footnote{Alcune prove si possono trovare alla pagina: \url{https://github.com/albertopasqualetto/Tesi-triennale/blob/main/Notebooks/chromaprint_comparisons.ipynb}}   % non è più stata trovata una differenza in base al codec

La libreria pyacoustid, nelle prime fasi di scrittura dei test, non era ancora completamente funzionante e compresa del metodo per comparare le impronte digitali, infatti la sua versione 1.2.2, presente sulla repository \href{https://pypi.org/}{PyPi}, non vedeva ancora implementata la funzione \verb|compare_fingerprints|, già presente invece sulla sua repository GitHub grazie al contributo di un utente; tale implementazione era, però, non funzionante a causa di alcuni errori del codice come si può vedere dai cambiamenti applicati successivamente e descritti qui: 

\begin{lstlisting}[language=diff]
@@ -382,7 +382,7 @@ def _match_fingerprints(a: List[int], b: List[int]) -> float:
            if biterror <= MAX_BIT_ERROR:
                offset = i - j + bsize
                counts[offset] += 1
-   topcount = counts.max()
+   topcount = max(counts)
    return topcount / min(asize, bsize)


@@ -397,8 +397,8 @@ def compare_fingerprints(a, b) -> float:
        raise ModuleNotFoundError("function needs chromaprint")

    # decompress fingerprints
-   a = [int(x) for x in chromaprint.decode_fingerprint(a)[0]]
-   b = [int(x) for x in chromaprint.decode_fingerprint(b)[0]]
+   a = [int(x) for x in chromaprint.decode_fingerprint(a[1])[0]]
+   b = [int(x) for x in chromaprint.decode_fingerprint(b[1])[0]]
    return _match_fingerprints(a, b)
\end{lstlisting}

Per applicare queste modifiche è stato necessario effettuare una \href{https://github.com/beetbox/pyacoustid/pull/78}{\ac{PR}}\footnote{Una \ac{PR} è una richiesta ai manutentori del codice aggiungerne di proprio per implementare nuove funzioni o per correggere dei problemi.} sulla repository ufficiale a cui ha seguito uno scambio di messaggi con il manutentore e l'implementatore del codice; questa è una pratica comune nel mondo del software libero, il quale, tra i vari vantaggi, permette di leggere il codice sorgente e contribuire con dei miglioramenti alle tecnologie che si utilizzano.\footnote{\url{https://opensource.guide/how-to-contribute/\#improve-software-you-rely-on}}
Si è contribuito a pyacoustid anche con \href{https://github.com/beetbox/pyacoustid/pull/79}{un'altra \ac{PR}} che permette l'utilizzo della libreria anche su piattaforme con Windows, la quale è stata implementata collaborando col manutentore.

Ora la versione presente su PyPi è la 1.3.0, contenente tutte le modifiche citate.


\subsection{Come verificare l'uguaglianza tra video} \label{ssec:packager-video}    % ffmpeg e psnr
Per verificare l'uguaglianza tra gli stream video è stato utilizzato il \ac{PSNR} medio tra i frame dei 2 video, calcolato tramite FFmpeg.\footnote{\url{https://ffmpeg.org/ffmpeg-all.html\#psnr}}

Il \ac{PSNR} è una misura di qualità di un'immagine compressa rispetto all'originale; viene definito come il rapporto tra la massima potenza di un segnale (immagine originale) e la potenza del rumore rispetto alla sua rappresentazione (immagine compressa).
In questo caso si utilizzano al posto delle immagini originale e compressa, i frame dei 2 video da confrontare rispettivamente.

\begin{lstlisting}[language=Python, caption=Test di comparazione di due file audio tramite il psnr]
VIDEO_THRESHOLD = 25    # dB
# [...]
psnr_out = subprocess.run(["ffmpeg",    # video
                           "-i", input_pvf_path,
                           "-i", tmp_path / (files_name+"_PreservationAudioVisualFile_output_video.mov"),
                           "-filter_complex", "psnr",
                           "-f", "null",
                           "-"],
                          capture_output=True)
psnr_out = psnr_out.stderr.decode("utf-8")
psnr = psnr_out[psnr_out.find('average:') + 8:psnr_out.find(' ', psnr_out.find('average:'))]
print("psnr_out=", psnr)
assert psnr == 'inf' or float(psnr) > VIDEO_THRESHOLD, "PreservationAudioVisualFile.mov is not the same as input"
\end{lstlisting}

Come si può osservare dal codice, è stato individuato un \ac{PSNR} soglia di \qty{25}{\dB}, ancora una volta non è massimo (infinito) perchè nei casi di sincronizzazione in cui si taglia una parte di video, si ottiene un valore minore (e nemmeno elevato) di \ac{PSNR}, ma comunque in grado di discernere video "uguali" da video veramente distinti.

Esistono altri metodi per comparare dei video e tra questi vi sono il \ac{SSIM} oppure, ancora una volta, qualche algoritmo di fingerprinting, ma si è visto che il \ac{PSNR} è funzionale allo scopo ed è il più rapido ad essere eseguito; il codice potrebbe essere migliorato per avere più certezza dell'uguaglianza, ma questo non è un problema semplice considerando che i video in questione sono identici a meno di un'eventuale taglio della parte iniziale; un'idea potrebbe essere quella di calcolare il psnr a ritroso e considerando i video solo per la durata del più breve, ciò però non considererebbe il caso limite di file più lunghi con la parte terminale in comune.


\subsection{Pulizia/reformat del codice della libreria} \label{ssec:packager-post}  % principio DRY, docstrings, unit tests, compatiblità Windows ma esecuzione docker
A termine del lavoro è stato eseguito un reformat del codice della libreria, nello stile del \ac{TDD}.
Si è cercato di seguire il principio \ac{DRY}, ovvero di limitare la ridondanza del codice creando delle funzioni autoesplicative per ogni macro azione del codice.
Facendo ciò si è limitata la ridondanza sia all'interno dello stesso file, sia si è rimossa la duplicazione tra client e server, che ora utilizzano entrambi le funzioni di una libreria condivisa.

Per le funzioni create precedentemente sono stati creati degli unit tests per le funzionalità non ancora testate dai conformance tests.

Ove opportuno sono state scritte le docstings per le funzioni, ovvero la loro documentazione.

Sono state anche aggiunte delle specifiche del comportamento per assicurare l'esecuzione da capo a fine in Windows, dato che alcune funzionalità non sono supportate dal sistema operativo di Microsoft:
\begin{lstlisting}[language=diff]
- quit(os.EX_CONFIG)
+ if not sys.platform.startswith(('win', 'cygwin')):
+     quit(os.EX_CONFIG)  # `os.EX_CONFIG` is not compatible with the above platforms in Python 3.10
+ else:
+     quit()
\end{lstlisting}
Ciò è stato fatto per assicurare l'esecuzione multipiattaforma, nonostante in produzione il packager sia eseguito in un container linux.


\section{\acs{CAE}-\acs{ARP} audio analyser} \label{sec:test-audioanalyser}
Successivamente è stato preso in considerazione l'\ac{AIM} Audio Analyser.

\begin{figure}
    \centering
    \includegraphics{img/audioanalyser.png}
    \caption{\ac{ARP} Audio Analyser}
    \label{fig:audio-analyser}
\end{figure}

L'audio analyser, come anticipato nella sottosezione \ref{ssec:mpai-cae-arp}, è l'\ac{AIM} che si occupa di individuare le irregolarità nell'audio, di fornirle al video analyser ed, una volta ottenute anche le irregolarità del video, di fornirle al tape irregularity classifier (il classificatore delle irregolarità), oltre a ciò si occupa di passare al classificatore appena citato anche dei frammenti di audio lunghi \qty{500}{\ms} in corrispondenza di ogni irregolarità, vedi schema in figura \ref{fig:audio-analyser}.

Viene riportata la descrizione dei conformance tests da seguire, definita da MPAI, nella tabella \ref{tab:audioanalyser-valutazione}.\footnote{Estratta dalla versione WD 0.15.2 del documento MPAI Conformance Testing per \ac{CAE}.}

\begin{table}[h]
    \centering
    \begin{tabular}{|p{0.2\textwidth}|p{0.8\textwidth}|}
        \hline
        \textbf{Means}   &   \textbf{Actions}\\
        \hline
        \textbf{Conformance Testing Dataset}    &
            DS1: n* Preservation Audio Files.\newline
            DS2: n Preservation Audio-Visual Files related to DS1.\newline
            DS3: n Irregularity Files related to DS2.\newline
            DS4: n output Irregularity Files in the format of port IrregularityFileOutput\_1 with all Irregularities correctly identified.\newline
            DS5: n output Irregularity Files in the format of port IrregularityFileOutput\_2 with the real offset and all Irregularities correctly identified and included from DS3.\newline
            \newline
            \footnotesize* A reasonable n for testing is 5<n<=10, since each file generates multiple irregularities to classify\\
        \hline
        \textbf{Procedure}  &
            1.	Feed Audio Analyser under test with DS1, DS2 and DS3.\newline
            2.	Compare the computed offsets with the ones contained in DS5.\newline
            3.	Analyse the Irregularity Files resulting from port IrregularityFileOutput\_1.\newline
            4.	Analyse the Irregularity Files resulting from port IrregularityFileOutput\_2.\\
        \hline
        \textbf{Evaluation} &
            	1.	Verify the conditions:\newline
            	   a.	The Irregularity Files are syntactically correct and conforming to the JSON schema provided in CAE Technical Specification.\newline
            	   b.	All Irregularities from DS3 are included in the Irregularity Files coming from port IrregularityFileOutput\_2.\newline
            	   c.	$|O_c - O_r| < 3 \times \lceil\frac{1000}{FPS_{DS3}}\rceil ms$, where $O_c$ is the offset computed by the Audio Analyser under test, $O_r$ is the real offset and $FPS_{DS3}$ is the number of frames per second at which the DS3 video has been recorded.\newline
            	   d.	All output Audio Files are conforming to RF64 file format.\newline
            	   e.	For each of the n tuples of input records, the output Audio Files are extracted from the corresponding input Preservation Audio File at the Time Labels indicated in the Irregularity File coming from port IrregularityFileOutput\_2.\newline
            	2.	By inspecting the Irregularity Files resulting from port IrregularityFileOutput\_1, for each of the n tuples of input records, compute the values of Recall (R) and Precision (P).\newline
            	3.	Compute the average value of Recall ($\Tilde{R}$) and Precision ($\Tilde{P}$) measures obtained at point 2.\newline
            	4.	Accept the AIM under test if:\newline
                	a.	$\Tilde{R} > 0.9$\newline
                	b.	$\Tilde{P} > 0.9$\\
        \hline
    \end{tabular}
    \caption{Test di conformità per \ac{ARP} Audio Analyser}
    \label{tab:audioanalyser-valutazione}
\end{table}

Ad ogni esecuzione dei test è richiesto di inserire i dati nella tabella \ref{tab:audioanalyser-report}.    % FIXME sistemare, solo copiato
\begin{table}[H]
    \centering
    \begin{adjustbox}{max width=\textwidth}
        \begin{tabular}{|c|c|}
            
            \hline
            \textbf{Conformance Tester ID}                      &   Unique Conformance Tester Identifier assigned by MPAI\\
            \hline
            \textbf{Standard, Use Case ID and Version}          &   Standard ID and Use Case ID, Version and Profile of the standard in the form “CAE:ARP:1:0”.\\
            \hline
            \textbf{Name of AIM}                                &   Audio Analyser\\
            \hline
            \textbf{Implementer ID}                             &   Unique Implementer Identifier assigned by MPAI Store.\\
            \hline
            \textbf{AIM Implementation Version}                 &   Unique Implementation Identifier assigned by Implementer.\\
            \hline
            \textbf{Neural Network Version (optional)}          &   Unique Neural Network Identifier assigned by Implementer.\\
            \hline
            \textbf{Identifier of Conformance Testing Dataset}  &   Unique Dataset Identifier assigned by MPAI Store.\\
            \hline
            \textbf{Test ID}                                    &   Unique Test Identifier assigned by Conformance Tester.\\
            \hline
            \textbf{Actual output}                              &   Actual output provided as a matrix of n rows containing R and P values.\\
                                                                &   \begin{tabular}{|c|c|c|}
                \hline
                Tuple \# &   R   &   P\\
                \hline
                1       &   Measure 1   &   Measure 1\\
                \hline
                \dots   &   \dots       &   \dots\\
                \hline
                n       &   Measure n   &   Measure n\\
                \hline
            \end{tabular}\\
            \hline
            \textbf{Execution time (optional)}                  &   Duration of test execution.\\
            \hline
            \textbf{Test comment (optional)}                    &   -\\
            \hline
            \textbf{Test Date}                                  &   yyyy/mm/dd.\\
            \hline
        \end{tabular}
    \end{adjustbox}
    \caption{Report da compilare ad ogni esecuzione dell'audio analyser.}
    \label{tab:audioanalyser-report}
\end{table}
Il report viene compilato automaticamente in un file JSON come spiegato anche per il packager (sottosezione \ref{ssec:packager-pre}).
Un esempio è riportato di seguito:
\lstinputlisting[language=json]{listings/audioanalyser-report.json}


\subsection{Problemi pre-esistenti} \label{ssec:audioanalyser-pre}  % typos, video analyser usa : invece di . e workaround, in Windows scipy.signal.correlate da overflow perche tipo di default per numpy è int32, test non funzionanti
Alla prima esecuzione del modulo, ancora prima di scrivere i test richiesti, si è notato che l'offset calcolato è estremamente diverso da quello fornito nei dataset di esempio, nello specifico si osservava addirittura uno scostamento di segno opposto, indagando si è individuato il problema nell'utilizzo su piattaforme Windows dato dal fatto che il tipo di dato di default per numpy in quest ultima è \texttt{int32}, mentre in ambiente Unix (quindi ancora una volta si far riferimento a problemi non riscontrabili in produzione) il default è \texttt{int64}, passando il tipo richiesto manualmente, il problema di overflow alla chiamata di \texttt{scipy.signal.correlate} non si presenta più.\footnote{\url{https://github.com/numpy/numpy/issues/9464}}
Questa problematica risiede nella libreria \texttt{mpai-cae-arp}, la quale viene discussa nella sezione \ref{sec:lib-mpaicaearp}.

Sono stati sistemati degli errori di battitura, alcuni dei quali impedivano il funzionamento del codice (vedi l'uso ambiguo di pydantic precedentemente permesso alla sottosezione \ref{ssec:mpaicaearp-pydantic})

È stata aggiornata la versione richiesta della libreria mpai-cae-arp:
\begin{lstlisting}[language=diff]
@@ -21,5 +21,5 @@ audio-analyser = 'audio_analyzer.cli:main'
    pandas = "^2.0.0"
    scikit-learn = "^1.2.2"
    grpcio-tools = "^1.53.0"
-   mpai-cae-arp = "^0.3.0"
+   mpai-cae-arp = "^0.4.1"
    ffmpeg-python = "^0.2.0"
\end{lstlisting}
su cui sono basati tutti gli \ac{AIM} di \ac{ARP}, si nota che la specifica \verb|^0.3.0| (basata sul \href{https://semver.org/lang/it/}{versionamento semantico}) permette solo le versioni $\ge0.3.0, <0.4.0$, quindi l'aggiornamento fino a prima della successiva MINOR, differendo dal normale funzionamento con versioni con MAJOR$\ge1$ ($\ge1.0.0$) con cui si permette l'aggiornamento fino a prima della successiva MAJOR.\footnote{Fonte: \href{https://python-poetry.org/docs/dependency-specification/\#caret-requirements}{documentazione di Poetry}.}

Infine nel codice dei test si dovrà fronteggiare il fatto che, in maniera incorretta, nel video analyser si utilizzino come divisore tra secondi e millisecondi i due punti (:) (si può vedere nei file \verb|*_IrregularityFileOutputN.json|), quindi, alla lettura di file contenenti irregolarità provenienti dal video analyser, si dovranno applicare delle soluzioni per aggirare il problema ed inoltre si vedranno fallire i test che chiedono di verificare la sintassi dello schema JSON nel caso contengano le irregolarità già citate.


\subsection{Come verificare che l'offset scelto è abbastanza vicino a quello reale} \label{ssec:audioanalyser-offset}   % formula fornita + ffprobe
Nella tabella di valutazione \ref{tab:audioanalyser-valutazione} al punto 1.c viene richiesto di controllare che l'offset calcolato dall'audio analyser sia compatibile con quello reale, fornito nel dataset 5.
Per fare ciò si richiede che la differenza in valore assoluto tra i 2 offset sia minore del tempo necessario a mostrare 3 frame del PreservationAudioVisualFile in millisecondi e si calcola tramite la seguente disuguaglianza:
\begin{equation}
    |O_c - O_r| < 3 \times \lceil\frac{1000}{FPS_{DS3}}\rceil ms
\end{equation}
dove $O_c$ è l'offset calcolato, $O_r$ è l'offset reale e $FPS_{DS3}$ sono gli FPS del PreservationAudioVisualFile.

Gli FPS del video sono ottenuti tramite \href{https://ffmpeg.org/ffprobe.html}{ffprobe}, una utility di FFmpeg votata all'acquisizione di informazioni da file multimediali, tramite la richiesta di mostrare lo stream \verb|r_frame_rate|.\footnote{Si usa il comando \verb|ffprobe -v 0 -of csv=p=0 -select_streams v -show_entries stream=r_frame_rate video_path.mov|}
\begin{lstlisting}[language=Python]
@pytest.mark.parametrize("files_name", files_names)
def test_offset_difference(files_name: str):
    """Conformance testing specification point c.

    Test if the offset difference between Audio Analyser and DS5 real offset is small.
    """
    aa_if1_path = temp_path / files_name / "AudioAnalyser_IrregularityFileOutput1.json"
    test_if2_path = oldify(temp_path / files_name / "AudioAnalyser_IrregularityFileOutput1.json")
    assert aa_if1_path.is_file(), "AudioAnalyser_IrregularityFileOutput1.json not found"
    with open(aa_if1_path, "r") as aa_if_file:
        aa_if_dict = json.load(aa_if_file)
    with open(test_if2_path, "r") as test_if_file:
        test_if_dict = json.load(test_if_file)
    aa_offset = int(aa_if_dict["Offset"])
    real_offset = int(test_if_dict["Offset"])

    out = subprocess.run(["ffprobe", "-v", "0", "-of", "csv=p=0", "-select_streams", "v", "-show_entries", "stream=r_frame_rate", str(working_path / "PreservationAudioVisualFile" / (files_name+".mov"))], capture_output=True)
    rate = out.stdout.decode("utf-8").split('/')
    fps = int(rate[0]) / int(rate[1])
    
    assert abs(aa_offset - real_offset) < 3 * math.ceil(1000 / fps), f"Offset difference is too big: {abs(aa_offset - real_offset)}ms"
\end{lstlisting}


\subsection{Come verificare che i file siano wav} \label{ssec:audioanalyser-wav}    % RF64 ma in realtà va bene wav; libreria filetype, magic numbers, MIME
Nella tabella \ref{tab:audioanalyser-valutazione} al punto 1.d viene richiesto di controllare che i segmenti audio estratti dall'audio analyser siano in formato RF64, un formato compadibile col wave ed in realtà è sufficiente verificare che tali segmenti siano in formato wave.

Per fare ciò è stata utilizzata la libreria Python \href{https://github.com/h2non/filetype.py}{filetype} che legge i magic numbers, ovvero delle stringhe costanti nell'header di un file, utilizzati per identificarne la tipologia.
Nel caso di wav, la sua firma in formato esadecimale è "\texttt{52 49 46 46 ?? ?? ?? ??
57 41 56 45}".\footnote{Gli \texttt{??} indicano che i valori in quelle posizioni non sono fissi.}
La libreria citata cerca di indovinare il tipo del file e ne restituisce il tipo \acs{MIME}, \texttt{audio/x-wav} nel caso del wave.\footnote{\acf{MIME} è uno standard che estende le email e fornisce il supporto a dati diversi dal solo testo. Il suo \texttt{Content-Type} indica il tipo di dato.}


\subsection{Come verificare che la classificazione sia coerente} \label{ssec:audio-analyser-classificazione}    % non necessario ma utile per capire che l'IA non da sempre stessi risultati, output utilizzati per gli altri test, recall, precision
Per verificare se la rilevazione e la classificazione delle irregolarità sono coerenti con il dataset di test, è stata creata la funzione \verb|check_classification_results| che confronta 2 dizionari contenenti irregolarità e restituisce il numero di elementi diversi e dove si trovano queste diversità.
\begin{lstlisting}[language=Python]
def check_classification_results(if_dict_1: dict, if_dict_2: dict, print_differences: bool = False) -> (bool, int, int, int):
    """
    Check if the irregularity files' classification results in `if_dict_1` are exactly equal to `if_dict_2`.

    Parameters
    ----------
    if_dict_1 : dict
        First irregularity file dictionary (first file to compare).
    if_dict_2 : dict
        Second irregularity file dictionary (second file to compare).
    print_differences : bool, optional
        If True, print the differences between the two files, by default False.

    Returns
    -------
    tuple[bool, int, int, int]
        Tuple of:
            1) if the two files are exactly equal;
            2) number of elements in common;
            3) number of extra elements in the first file;
            4) number of extra elements in the second file.
    """
    def check_all_combinations(list1: list[dict], idxs1: list[int], list2: list[dict], idxs2: list[int]) -> (int | None, int | None):
        """
        Check if any combination of 2 elements with specified indexes in two different lists is equal and return the indexes of the first equal elements found.
        # [...]
        """
        def filtered_dict(dict_obj: dict, filter: list) -> dict:
            # [...]

        # [...]

    exact_equality = True
    found_n = 0
    extra_in_1 = 0
    extra_in_2 = 0
    irrs_1 = if_dict_1["Irregularities"]
    irrs_1.sort(key=lambda irr: int(irr["TimeLabel"].replace(":", "").replace(".", "")))  # FIXME workaround for ":" timelabel problem from video analyser
    irrs_2 = if_dict_2["Irregularities"]
    irrs_2.sort(key=lambda irr: int(irr["TimeLabel"].replace(":", "").replace(".", "")))

    timelabels_1 = [irr["TimeLabel"] for irr in irrs_2]
    for timelabel in list(dict.fromkeys(timelabels_1)):     # remove duplicates
        irrs_1_idxs = get_irr_idxs_by_timelabel(irrs_1, timelabel)
        irrs_2_idxs = get_irr_idxs_by_timelabel(irrs_2, timelabel)

        while True:
            irr_1_idx, irr_2_idx = check_all_combinations(irrs_1, irrs_1_idxs, irrs_2, irrs_2_idxs)
            if irr_1_idx or irr_2_idx:  # first check exact equality
                pass
            else:  # then the others remained as couples (equal length already checked)
                if not len(irrs_1_idxs) or not len(irrs_2_idxs):
                    break
                exact_equality = False
                irr_1_idx = irrs_1_idxs[0]
                irr_2_idx = irrs_2_idxs[0]
                if print_differences:
                    print("Irregularity", irrs_1[irr_1_idx], "\n\tnot exactly equal to\n\t", irrs_2[irr_2_idx])
            found_n += 1
            del irrs_1[irr_1_idx]
            del irrs_2[irr_2_idx]
            irrs_1_idxs.remove(irr_1_idx)
            irrs_2_idxs.remove(irr_2_idx)
            irrs_1_idxs = [idx - 1 if idx > irr_1_idx else idx for idx in irrs_1_idxs]
            irrs_2_idxs = [idx - 1 if idx > irr_2_idx else idx for idx in irrs_2_idxs]
        if len(irrs_2_idxs):
            extra_in_2 += len(irrs_2_idxs)

    extra_in_1 += len(irrs_1)

    return exact_equality, found_n, extra_in_1, extra_in_2
\end{lstlisting}

Quando si ha a che fare con algoritmi basati sul machine learning è la norma avere risultati non deterministici per via della componente di casualità che spesso vi è inserita.
Infatti il test che verifica l'esatta uguaglianza tra le 2 collezioni di irregolarità (\verb|test_audio_classification|) lo si è sempre visto fallire durante le sue esecuzioni, infatti esso non è richiesto dal documento descrivente i test di conformità.
Ciò che è richiesto e che si basa sul codice appena citato, invece, è di controllare che tutte e sole le irregolarità presenti nel file di test siano presenti anche nel file in output ad una nuova esecuzione dell'audio analyser e che siano correttamente estratti i frammenti di audio agli istanti di tempo richiesti (punto 1.e di \ref{tab:audioanalyser-valutazione}) e che inoltre la precisione ed il recupero medi delle irregolarità trovate all'istante di tempo ed ai canali a cui appartengono rispetto al dizionario delle irregolarità di riferimento, il dataset 4, siano entrambi $>0.9$ (punto 2 di \ref{tab:audioanalyser-valutazione}).\footnote{Precisione e recupero (\textit{Precision} e \textit{Recall}) sono entrambe metriche sulle prestazioni di una classificazione: $Precision=\frac{True\ Positives}{True\ Positives + False\ Positives}$; $Recall=\frac{True\ Positives}{True\ Positives + False\ Negatives}$.}
% TODO scrivere meglio


\section{Parallelizzare o no? confronto di velocità} \label{sec:parallelizzazione}
% TODO fare test





\section{Libreria \texttt{mpai-cae-arp}} \label{sec:lib-mpaicaearp} % a cosa serve
La libreria \href{https://pypi.org/project/mpai-cae-arp/}{mpai-cae-arp} contiene le classi, le costanti e le funzioni su cui si basano tutti gli \acp{AIM} di \ac{ARP} e che permettono loro di interoperare.
Per questo motivo alcuni problemi relativi a questa libreria sono stati discussi in altre sezioni dato la sua ampia influenza (vedi \ref{ssec:audioanalyser-pre}).
% TODO scrivere altro?


\subsection{Bug: Pydantic} \label{ssec:mpaicaearp-pydantic}    % che ha portato a dover aggiornare i moduli
È stato risolto un problema relativo all'uso "ambiguo" di \href{https://docs.pydantic.dev/1.10/}{Pydantic}, il suo scopo principale all'interno di \ac{ARP} è quello di disporre di alias per i parametri delle funzioni in maniera da utilizzare il nome dell'alias per l'output verso file (\texttt{EditingList.json}, \verb|*_IrregularityFileOutputN.json|, \dots), seguendo la nomenclatura richiesta dalle specifiche tecniche; in precedenza l'utilizzo del nome del parametro o del suo alias non era regolato e questo ha portato a degli errori che impedivano la corretta esecuzione del software.
La risoluzione consiste nell'utilizzo di \verb|serialization_alias| al posto di \texttt{alias}, dato l'utilizzo primario degli alias.

Contestualmente, per favorire la futuribilità dell'applicativo, oltre a limitarne la presenza dei bug, si è scelto di aggiornare la libreria alla successiva major release (\verb|^2.3.0|).

Queste modifiche implicano il dover aggiornare i nomi dei campi utilizzati in tutti gli \acp{AIM} impedendo la retrocompatibilità.


\subsection{Bug: formatting sbagliato delle EditingList scritte su file} \label{ssec:mpaicaearp-editinglist-format}
Un bug presente nel codice di questa libreria salvava nel filesystem dei file con una formattazione errata, un esempio ne è l'\texttt{EditingList.json} che veniva salvato nella seguente maniera:
\lstinputlisting[language=json, caption={\texttt{EditingList.json} salvato tramite la precedente modalità, contiene solo una stringa}]{listings/EditingList-badformat.json}
Il JSON di cui sopra è formato da una sola stringa che contiene la rappresentazione dell'oggetto Python ed i caratteri di escape mostrati, ciò portava al fallimento del relativo test di conformità del packager (\ref{sec:test-packager}.

Il problema risiedeva nella chiamata di \texttt{json.dump} sulla stringa contenente l'oggetto da salvare in JSON generata da \texttt{model.json} (in Pydantic V2 chiamata \verb|model.model_dump_json|), questo comportamento probabilmente non era chiaro all'autore di questo codice per via del suo nome fuorviante; utilizzando, invece, la funzione di Pydantic V2 \verb|model.model_dump|, chiamata con parametro \texttt{mode='json'} per abilitare la serializzazione degli oggetti presenti nei campi della classe, si ottiene un dizionario Python che può essere scritto correttamente su file JSON (in Pydantic V1 la funzione corrispondente sarebbe stata \texttt{model.dict}, ma essa non fornisce la possibilità di serializzare gli oggetti).
Ora il file precedente appare correttamente come:
\lstinputlisting[language=json, caption={\texttt{EditingList.json} salvato tramite l'attuale modalità, contiene il dizionario con i suoi valori serializzati}]{listings/EditingList-goodformat.json}


\subsection{Bug: test di \texttt{AudioWave} a singolo canale non funzionante e fix di \texttt{get\_channel}}    % e fare riferimento a dtype del suo buffer nominato sopra





\subsection{Aggiornamento delle librerie perchè cross-dependencies ora supportate (librosa, llvmlite, numpy)}
\subsection{Aggiornamento a Python 3.11} \label{ssec:py-311}
